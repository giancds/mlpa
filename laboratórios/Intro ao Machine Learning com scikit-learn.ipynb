{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Objetivo\n",
    "\n",
    "A intenção deste exercício é observar as diferenças de aprendizado entre os algoritmos e como os resultados podem variar entre eles. \n",
    "\n",
    "Neste primeiro momento nós vamos utilizar funções padrão da biblioteca scikit-learn para estudar os algoritmos básicos de machine learning. Como nossa atividade é bastante simples vamos utilizar datasets que vem pré-construídos na biblioteca.\n",
    "\n",
    "\n",
    "\n",
    "## 2 - Carregando as bibliotecas\n",
    "\n",
    "Scikit-learn possui uma interface limpa e intuitiva e todos os componentes da biblioteca expõem a mesma interface de métodos. Sendo uma biblioteca fácil de usar, tornou-se padrão na indústria de tecnologia.\n",
    "\n",
    "Para utilizá-la, vamos primeiro carregar os métodos/módulos necessários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt  # primeiro importamos a biblioteca para visualização\n",
    "import numpy as np  # importamos também a biblioteca NumPy que irá nos fornecer diversos métodos para trabalhar com arrays\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de iniciarmos o nosso problema, nós também devemos definir algumas funções que nos auxiliarão no tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_iris(X):\n",
    "    \"\"\" Função para visualização do dataset iris. Embora seja um dataset \n",
    "    com 4 features, vamos utilizar apenas as 2 primeiras para gerar um \n",
    "    gráfico\n",
    "    \"\"\"\n",
    "    plt.figure(2, figsize=(8, 6))\n",
    "    plt.clf()\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Set1,\n",
    "            edgecolor='k')\n",
    "    plt.xlabel('Sepal length')\n",
    "    plt.ylabel('Sepal width')\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def plot_resultado(X_, y_, clf, title):\n",
    "    X = X_[:, :2]\n",
    "    y = y_\n",
    "\n",
    "    h = .02  # step size in the mesh\n",
    "    # we create an instance of Neighbours Classifier and fit the data.\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    \n",
    "    preds = clf.predict(X)\n",
    "    acc = accuracy_score(y, preds)\n",
    "    print(\"Percentual de acertos {:.2f}\".format(acc))\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure()\n",
    "    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,\n",
    "                edgecolor='k', s=20)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 - Dataset\n",
    "\n",
    "Nós vamos utilizar o *Iris flower dataset* (ou *Fisher's Iris dataset*), criado pelo estatístico e biólogo britânico Ronald Fisher em seu artigo \"*The use of multiple measurements in taxonomic problems*\", de 1936, no qual apresentou a primeira versão da análise de discriminantes lineares.\n",
    "\n",
    "Este dataset contém 50 exemplos de três espécies de flores de Íris (*Iris setosa*, *Iris virginica* e *Iris versicolor*). Cada datapoint consiste de 4 features: comprimento e largura da sépala, comprimento e largura das pétalas (todos em centímetros).\n",
    "\n",
    "Este dataset se tornou um caso de teste típico para algoritmos de machine learning com foco em classificação.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets # aqui importamos o módulo de datasets\n",
    "\n",
    "# aqui nós carregamos o dataset\n",
    "iris = datasets.load_iris() \n",
    "\n",
    "# nós vamos dividir nosso dataset em X (features) e y (alvo)\n",
    "X = iris.data \n",
    "y = iris.target\n",
    "\n",
    "# vamos agora dividir o dataset em treino e teste\n",
    "indices = np.random.permutation(len(X))  # primeiro obtemos uma lista de ordem aleatória dos índices do dataset\n",
    "X_train = X[indices[:-10]]  # obtemos uma parte das features para treino\n",
    "y_train = y[indices[:-10]]  # obtemos a parte correspondente dos alvos para treino\n",
    "X_test  = X[indices[-10:]]  # obtemos uma parte das features para teste\n",
    "y_test  = y[indices[-10:]]  # obtemos a parte correspondente dos alvos para teste\n",
    "\n",
    "# criando uma visualização básica\n",
    "sns.pairplot(pd.read_csv(\"data/iris.csv\"), hue=\"species\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver acima, nosso problema possui 3 classes a serem classificadas. Uma delas aparenta ser simples, ao passo que as outras estão um pouco embaralhada!\n",
    "\n",
    "*Observação*: devido ao fato de conseguirmos criar um gráfico em 2 dimensões, nós vamos utilizar apenas as 2 primeiras features neste exercício. É importante lembrar que este exercício tem a intenção de nos dar algumas intuições sobre o machine learning e o funcionamento de seus algoritmos e não de ser uma representação exata de suas capacidades.\n",
    "\n",
    "## 3 - Os modelos\n",
    "\n",
    "O scikit-learn é uma biblioteca que implementa (quase) todos os principais algoritmos usados em machine learning, assim como diversos métodos utilitários para feature engineering e feature selection.\n",
    "\n",
    "O principal método que todo o algoritmo implementa é o  ```fit```. Desta forma, fica fácil criarmos/substituirmos um algoritmo:\n",
    "\n",
    "```\n",
    "algoritmo.fit(features, alvos)\n",
    "```\n",
    "\n",
    "O scikit-learn implementa diversos parâmetros para cada algoritmo e, em geral, possui um valor default para todos eles. Mesmo assim, podemos alterar esses valores default usando o padrão python ```param=valor```:\n",
    "\n",
    "\n",
    "```\n",
    "algoritmo = Algoritmo(parametro1=valor1, parametro2=valor2, ...)\n",
    "```\n",
    "\n",
    "Cada algoritmo possui uma série de parâmetros que em geral diferem uns dos outros. Para verificar quais parâmetros são implementados por cada algoritmo e o significado de cada um, podemos consultar o link http://scikit-learn.org/stable/modules/classes.html\n",
    "\n",
    "#### 3.1 - Decision Trees\n",
    "\n",
    "Agora vamos treinar o nosso primeiro modelo usando scikit-learn: uma árvore de decisão (decision tree). Este modelo é o padrão dos métodos baseados em informação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dectree = DecisionTreeClassifier(criterion=\"gini\", \n",
    "                                 splitter=\"best\", \n",
    "                                 max_depth=None, \n",
    "                                 min_samples_split=2, \n",
    "                                 min_samples_leaf=1, \n",
    "                                 min_weight_fraction_leaf=0.0, \n",
    "                                 max_features=None, \n",
    "                                 random_state=None, \n",
    "                                 max_leaf_nodes=None, \n",
    "                                 min_impurity_decrease=0.0, \n",
    "                                 min_impurity_split=None, \n",
    "                                 class_weight=None, \n",
    "                                 presort=False)\n",
    "\n",
    "plot_resultado(X, y, dectree, \"Árvore de decisão\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - k-nearest neighbours\n",
    "\n",
    "Agora que já observamos nosso primeiro algoritmo baseado em informação, vamos passar aos algoritmos baseados em similaridade. O padrão para estes algoritmos é o K-nearest neighbours. Este algoritmo mede a distância entre os pontos no feature space e utiliza os K datapoints mais próximos para determinar qual a classificação. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5, \n",
    "                           weights=\"uniform\", \n",
    "                           algorithm=\"auto\", \n",
    "                           leaf_size=30, \n",
    "                           p=2, \n",
    "                           metric=\"minkowski\", \n",
    "                           metric_params=None, \n",
    "                           n_jobs=1)\n",
    "\n",
    "plot_resultado(X, y, knn, \"K-Nearest Neighbours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 - Naïve Bayes\n",
    "\n",
    "O próximo algoritmo que vamos analisar é o Naïve Bayes, padrão dos métodos baseados em probabilidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "nbayes = GaussianNB(priors=None)\n",
    "plot_resultado(X, y, nbayes, \"Naïve Bayes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 - Regressão Logística\n",
    "\n",
    "O último tipo de algoritmo que nos falta estudar é o baseado em erro. O método padrão é a Regressão Logística, similar ao método usado no ramo da estatística, com algumas alterações apenas na forma de se treinar o modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "linear = LogisticRegression(penalty=\"l2\", \n",
    "                            dual=False, \n",
    "                            tol=0.0001, \n",
    "                            C=1.0, \n",
    "                            fit_intercept=True, \n",
    "                            intercept_scaling=1, \n",
    "                            class_weight=None, \n",
    "                            random_state=None, \n",
    "                            solver=\"liblinear\", \n",
    "                            max_iter=100, \n",
    "                            multi_class=\"ovr\", \n",
    "                            verbose=0, \n",
    "                            warm_start=False, \n",
    "                            n_jobs=1)\n",
    "\n",
    "plot_resultado(X, y, linear, \"Regressão Logística\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Exercícios\n",
    "\n",
    "#### 4.1 - Testando os hyperparâmetros\n",
    "\n",
    "Volte aos exercícios acima e troque os hyperparâmetros *default* de cada algoritmo. Observe como o gráfico muda (ou não). O controle dos hyperparâmetros é um dos fatores que pode determinar qual o melhor algoritmo para cada caso. Na próxima lição nós vamos estudar um método de busca para otimizar a escolha dos hyperparâmetros.\n",
    "\n",
    "Para auxiliar na tarefa de escolha, veja a documentação de cada classe nos links disponíveis em cada seção.\n",
    "\n",
    "*Observação*: o único algoritmo que não possui hyperparâmetros é o baseado em probabilidades.\n",
    "\n",
    "#### 4.2 - Outros algoritmos\n",
    "\n",
    "Acesse o link abaixo e escolha outro(s) algoritmo(s) que ainda não foram vistos e treine um ou mais modelos com ele(s). Observe como o algoritmo se comporta.\n",
    "\n",
    "Link: http://scikit-learn.org/stable/modules/classes.html\n",
    "\n",
    "Você pode utilizar o código abaixo trocando a palavra-chave nome_do_algoritmo pelo escolhido. Não esqueça de verificar o nome da classe utilizada no ```import```!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.classe_do_algoritmo import nome_do_algoritmo\n",
    "classifier = nome_do_algoritmo()\n",
    "plot_resultado(X, y, classifier, \"Teste\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
