{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Objetivo\n",
    "\n",
    "Neste exercício vamos explorar a biblioteca tensorflow para construir redes neurais artificiais. A intenção é observar como uma biblioteca que contém funções prontas pode nos auxiliar a criar nossos models de maneira mais rápida.\n",
    "\n",
    "Novamente utilizaremos a classificação de imagens de números do dataset [MNIST](http://yann.lecun.com/exdb/mnist/) como exemplo. Desta forma, poderemos comparar o código gerado neste exercício com o código do exercício anterior.\n",
    "\n",
    "\n",
    "\n",
    "## 2 - Carregando as bibliotecas\n",
    "\n",
    "[Tensorflow](https://www.tensorflow.org/) é uma biblioteca de redes neurais para python desenvolvido inicialmente pelo Google. Atualmente é *open source* e possui uma base ativa de desenvolvimento.\n",
    "\n",
    "Além desta biblioteca, vamos utilizar um módulo chamado ```input_data``` criado especialmente para o propósito deste exercício."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow.contrib.eager as tfe\n",
    "import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Funções auxiliares\n",
    "\n",
    "Neste exercício, faremos uso de diversas funções auxiliares para melhor visualizar cada etapa de otimização da rede neural.\n",
    "\n",
    "Primeiro, temos uma função que vai carregar o dataset. A maior parte da lógica vem do módulo ```input_data```. A função vai retornar os datapoints divididos em dataset de treino e de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir):\n",
    "  \"\"\"Returns training and test tf.data.Dataset objects.\"\"\"\n",
    "  data = input_data.read_data_sets(data_dir, one_hot=True)\n",
    "  train_ds = tf.data.Dataset.from_tensor_slices((data.train.images,\n",
    "                                                 data.train.labels))\n",
    "  test_ds = tf.data.Dataset.from_tensors((data.test.images, data.test.labels))\n",
    "  return (train_ds, test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, vamos definir a função de custo que será utilizada para calcular o sinal de erro do algoritmo backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(predictions, labels):\n",
    "  return tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "#       tf.nn.softmax_cross_entropy_with_logits(\n",
    "          logits=predictions, labels=labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui definimos a função que vai calcular o percentual de acertos obtidos pela nossa rede neural."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(predictions, labels):\n",
    "  return tf.reduce_sum(\n",
    "      tf.cast(\n",
    "          tf.equal(\n",
    "              tf.argmax(predictions, axis=1,\n",
    "                        output_type=tf.int64),\n",
    "              tf.argmax(labels, axis=1,\n",
    "                        output_type=tf.int64)),\n",
    "          dtype=tf.float32)) / float(predictions.shape[0].value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A próxima função é aquela que irá executar a lógica de otimização da rede neural. Nela, nós temos que definir a lógica de gradiente descendente e backpropagation. Note como o código é enxuto. \n",
    "\n",
    "Para entender como ele funciona, atente aos comentários do código.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, dataset, log_interval=None):\n",
    "  \"\"\"Treina o modelo usando um 'dataset' com  um 'otimizador'.\"\"\"\n",
    "\n",
    "  #  esta linha serve para ativar ou buscar um contador interno da biblioteca que retorna o número do \n",
    "  # 'batch' executado - se existe, busca o existente, senão cria um novo\n",
    "  tf.train.get_or_create_global_step()  \n",
    "  \n",
    "  # este é o laço principal da otimização\n",
    "  for (batch, (inputs, alvos)) in enumerate(tfe.Iterator(dataset)):\n",
    "    # esta linha diz para a biblioteca guardar o estado da otimização a cada 10 'batches' contados por\n",
    "    # tf.train.get_or_create_global_step() criado acima\n",
    "    # desta forma, podemos visualizar o comportamento da otimização e entendermos como o modelo está funcionando e\n",
    "    # quais partes podem ser melhoradas\n",
    "    with tf.contrib.summary.record_summaries_every_n_global_steps(10):\n",
    "      # nesta linha, definimos uma 'fita' que vai armazenar a ordem de execução da rede neural e assim como\n",
    "      # os gradientes calculados para backpropagation\n",
    "      with tfe.GradientTape() as tape:\n",
    "        prediction = model(inputs, training=True)  # obtendo a saída da rede neural\n",
    "        loss_value = loss(prediction, alvos)  # calculando o sinal de erro\n",
    "        tf.contrib.summary.scalar('loss', loss_value)  # aqui, adicionamos o sinal de erro à vizualição\n",
    "        tf.contrib.summary.scalar('accuracy',\n",
    "                                  compute_accuracy(prediction, alvos)) # adicionamos também o percentual de acertos\n",
    "      # é aqui que a 'mágica' acontece\n",
    "      # tensorflow possui um módulo de auto-diferenciação que lê a 'fita' de execução e calcula todos as derivadas \n",
    "      # necessárias para o algoritmo de backpropagation, incluindo a execução do próprio algoritmo!\n",
    "      # desta forma, não precisamos nos preocupar com a lógica daquele algoritmo e podemos nos concentrar\n",
    "      # no modelo em si\n",
    "      grads = tape.gradient(loss_value, model.variables) \n",
    "      # uma vez executado o backpropagation, o optimizer irá aplicar a atualização de parâmetros\n",
    "      optimizer.apply_gradients(zip(grads, model.variables))\n",
    "      # por fim, vamos imprimir na tela o progresso da otimização\n",
    "      if log_interval and batch % log_interval == 0:\n",
    "        print('Batch #%d\\tLoss: %.6f' % (batch, loss_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após criarmos o laço de otimização, é a vez de criarmos o laço de teste. Assim como no laboratório anterior, não precisamos de uma lógica de backpropagation, apenas uma forma de calcular a performance da rede neural. O código é juito similar ao código de otimização.\n",
    "\n",
    "Para mais detalhes, veja os comentários do código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataset):\n",
    "  \"\"\"Realiza um teste do 'model' utilizando datapoins retirados do 'dataset'.\"\"\"\n",
    "  avg_loss = tfe.metrics.Mean('loss')  # aqui usamos uma função pré-construída para calcular a média do sinal de erro\n",
    "  accuracy = tfe.metrics.Accuracy('accuracy')  # e fazemos o mesmo com o percentual de acertos\n",
    "\n",
    "  # este é o laço principal do teste\n",
    "  for (images, labels) in tfe.Iterator(dataset):\n",
    "    predictions = model(images, training=False)  # obtemos as predições\n",
    "    avg_loss(loss(predictions, labels))  # calculamos a média do sinal de erro\n",
    "    accuracy(tf.argmax(predictions, axis=1, output_type=tf.int64),  \n",
    "             tf.argmax(labels, axis=1, output_type=tf.int64))  # calculamos o percentual de acertos\n",
    "  print('Test set: Average loss: %.4f, Accuracy: %4f%%\\n' %\n",
    "        (avg_loss.result(), 100 * accuracy.result()))  # imprimimos na tela algumas dessas informações\n",
    "\n",
    "  # por fim, nas linhas abaixo, adicionamos mais dados à visualização do treino\n",
    "  with tf.contrib.summary.always_record_summaries():\n",
    "    tf.contrib.summary.scalar('loss', avg_loss.result())\n",
    "    tf.contrib.summary.scalar('accuracy', accuracy.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Grafos e * eager execution*\n",
    "\n",
    "De maneira geral, cada vez que executamos um código escrito usando a biblioteca tensorflow, seguimos uma lista de passos:\n",
    "\n",
    "1. Carregamos o(s) dataset(s).\n",
    "2. Definimos o modelo que será treinado.\n",
    "3. Treinamos o modelo.\n",
    "4. Avaliamos sua performance.\n",
    "5. Usamos o modelo para fazer previsões/classificações.\n",
    "\n",
    "Esta biblioteca possui interfaces de alto e baixo nível que nos dão flexibilidade na hora de escrevermos o código do nosso modelo. Na maioria das vezes podemos utilizar interfaces de alto nível para as etapas 1, 3, 4 e 5. O ponto principal de variação se encontra na etapa 2.\n",
    "\n",
    "A biblioteca nos oferece 2 formas de executar os programas: através de grafos (preferida pela biblioteca) ou de forma imperativa (chamado pela biblioteca de modo *eager execution*).\n",
    "\n",
    "Na forma de grafos, nós precisamos primeiro definir a dependência de funções que compõem a rede neural (*i.e.*, as camadas). Uma vez feito isso, a biblioteca compila um programa que então será usado para treinar o modelo. Esta forma de definir a rede neural faz com que diversas otimizações internas (feitas pela própria biblioteca) sejam possíveis, especialmente na parte de auto-diferenciação e *backpropagation*, fazendo com que o programa seja executado mais rapidamente. \n",
    "\n",
    "No entanto, é demasiado difícil depurarmos um programa escrito desta forma. Nós não temos acesso ao grafo de dependências construído e precisamos encontrar eventuais erros de lógica usando muita intuição ou fazendo testes de mesa. Por isso, os mantenedores da biblioteca criaram uma forma diferente de definirmos nossa rede neural: modo *eager execution*.\n",
    "\n",
    "O modo *eager execution* segue uma forma imperativa, mais tradicional, de programação. Nós podemos criar a função e executá-la imediatamente, tendo assim, uma forma de acompanhar o fluxo do nosso código e facilitar a depuração. Para isso, a biblioteca cria uma \"fita\" de dependências que é escrita, lida e apagada a cada iteração. Desta forma, diversas otimizações que podem ser feitas na execução por grafos não são possíveis com *eager execution*.\n",
    "\n",
    "De maneira geral, recomenda-se iniciar a codificação de nossos modelos utilizando *eager execution* e, uma vez que tenhamos certeza de que tudo funciona como devido, trocarmos para o modo de grafos.\n",
    "\n",
    "\n",
    "#### 4.1 - Definindo a rede neural\n",
    "\n",
    "Abaixo vamos definir uma rede neural para classificar os dígitos do dataset MNIST. Esta estrutura é similar à rede LeNet5 de 1998, utilizada pelos correios dos Estados Unidos para auxiliar na leitura dos códigos de endereçamento postal. \n",
    "\n",
    "Esta rede possui 7 camadas:\n",
    "\n",
    "1. Input\n",
    "2. Convolução 1\n",
    "3. Maxpool 1\n",
    "4. Convolução 2\n",
    "5. MaxPool 2\n",
    "6. ReLu (aqui está a principal diferença em relação a original, que usava uma camada sigmoid)\n",
    "7. Linear (isto é, não possui uma função de ativação, apenas a transformação linear)\n",
    "\n",
    "Para detalhes, veja os comentários no código abaixo. Para uma visualização esquemática da rede, veja os slides do Dia 04, seção \"Redes Neurais Convolucionais\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTModel(tfe.Network):\n",
    "    \"\"\"MNIST Network.\n",
    "    \n",
    "        Para podermos executar nosso modelo no modo eager execution, nossa classe deve obrigatoriamente \n",
    "            herdar da classe tfe.Network.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_format):\n",
    "        super(MNISTModel, self).__init__(name='')\n",
    "        # esta parte define o formato dos dados de input - recomenda-se utilizar o padrão definido no código\n",
    "        # de treino - para descrições mais detalhadas, veja a documentação da classe tf.layers.Conv2D\n",
    "        if data_format == 'channels_first':\n",
    "            self._input_shape = [-1, 1, 28, 28]\n",
    "        else:\n",
    "            assert data_format == 'channels_last'\n",
    "            self._input_shape = [-1, 28, 28, 1]\n",
    "        # neste ponto definimos a estrutura da rede neural\n",
    "        # para que a 'fita\" de gradientes mantenha um registro das dependências e a biblioteca mantenha\n",
    "        # um registro dos parâmetros, devemos passar a definição de cada camada para a função self.track_layer\n",
    "        # que é definida em tfe.Network\n",
    "        # perceba que cada camada é definida por uma classe que pertence ao módulo tf.layers\n",
    "        # para as camadas Conv2d (Convoluções), nós precisamos definir o número de filtros e o tamanho do filtro -\n",
    "        # para definir o tamanho do filtro podemos passar um número inteiro (define mesma altura /largura) ou uma \n",
    "        # tupla que definirá altura/largura customizados\n",
    "        self.conv1 = self.track_layer(\n",
    "            tf.layers.Conv2D(filters=32, kernel_size=5, data_format=data_format, activation=tf.nn.relu))\n",
    "        self.conv2 = self.track_layer(\n",
    "            tf.layers.Conv2D(filters=64, kernel_size=5, data_format=data_format, activation=tf.nn.relu))\n",
    "        # para definir as camadas tradicionais, selecionamos a camada Dense e definimos o número de neurônios (units)\n",
    "        # na camada, assim como a função de ativação (activation) - no caso de não passarmos uma função de ativação \n",
    "        # como parâmetro, a camada se torna uma camada 'linear'- em outras palavras, a camada realiza apenas o cálculo\n",
    "        # da transformação linear sobre os inputs\n",
    "        self.fc1 = self.track_layer(tf.layers.Dense(units=1024, activation=tf.nn.relu))\n",
    "        self.fc2 = self.track_layer(tf.layers.Dense(units=10))\n",
    "        self.dropout = self.track_layer(tf.layers.Dropout(0.5))\n",
    "        # a camada de maxpool não possui parâmetros pois ela apelas realiza uma operação 'max' (seleciona o maior \n",
    "        # valor dentre uma lista de valores) e, desta forma, podemos criar apenas uma camada em nosso código e \n",
    "        # reutilizá-la para cada Conv2d. para definir esta camada, devemos definir o tamanho da 'janela' de valores e \n",
    "        # o deslocamento da 'janela' sobre o resultado da convolução. a definição da altura/largura segue o mesmo \n",
    "        # padrão utilizado na camada Conv2d\n",
    "        self.max_pool2d = self.track_layer(\n",
    "            tf.layers.MaxPooling2D(\n",
    "                pool_size=(2, 2), strides=(2, 2), padding='SAME', data_format=data_format))\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        \"\"\" Classifica os dígitos baseado nos 'inputs'.\n",
    "        \"\"\"\n",
    "        # para termos certeza de que os inputs estão no formato esperado, realizamos uma operação reshape\n",
    "        x = tf.reshape(inputs, self._input_shape)\n",
    "        # em seguida, aplicamos em ordem as camadas definidas na inicialização da classe\n",
    "        x = self.conv1(x)  # primeira convolução\n",
    "        x = self.max_pool2d(x)  # primeiro max-pool\n",
    "        x = self.conv2(x)  # segunda convolução\n",
    "        x = self.max_pool2d(x)  # segundo max-pool - nota: esta é a mesma função acima - ver comentários na definição\n",
    "        # como nós estamos operando sobre imagens, cada input será 2d (uma matriz). no entanto, as camadas \n",
    "        # Dense (camadas 'tradicionais') esperam um input 1d para cada exemplo. por isso, nós usamos uma função \n",
    "        # pré-contruida para fazer um reshape e transformar o input 2d em 1d - nota: nós poderíamos ter utilizado\n",
    "        # a função reshape como feito anteriormente, passando o formato correto.\n",
    "        x = tf.layers.flatten(x)\n",
    "        x = self.fc1(x)  # primeira camada dense\n",
    "        # aqui aplicamos uma função de dropout para auxiliar na regularização - o parâmetro training serve para\n",
    "        # ativar/desativar o dropout - ele é aplicado apenas durante o treino e não durante o teste\n",
    "        x = self.dropout(x, training=training) \n",
    "        x = self.fc2(x)  #  segunda camada dense (output)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 4.2 - O programa de treino\n",
    "\n",
    "Neste exercício nós vamos utilizar o modo *eager execution*  para observarmos e alterarmos o comportamento de nossa\n",
    "rede neural. O código para treinar a rede neural está definido abaixo, fazendo uso das funções auxiliares que criamos anteriormente.\n",
    "\n",
    "Para mais detalhes, atente aos comentários no código abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(_):\n",
    "  \n",
    "    # primeiro devemos habilitar o modo eager execution\n",
    "    # aviso: este comando deve ser executado apenas uma vez. caso contrário um exceção será lançada\n",
    "    tfe.enable_eager_execution()\n",
    "    \n",
    "    # aqui define-se o dispositivo que será utilizado para o treino da rede e o formato dos dados passados\n",
    "    # de maneira geral, usamos o formato de dados default como definidos abaixo\n",
    "    (device, data_format) = ('/gpu:0', 'channels_first')\n",
    "    if FLAGS.no_gpu or tfe.num_gpus() <= 0:\n",
    "        (device, data_format) = ('/cpu:0', 'channels_last')\n",
    "        print('Using device %s, and data format %s.' % (device, data_format))\n",
    "\n",
    "    # carregando os datasets em treino e teste e embaralhando os exemplos\n",
    "    # embaralhar os exemplos de treino auxilia a \"quebrar\" as dependências criadas pelo processamento\n",
    "    # ordenado do dataset\n",
    "    (train_ds, test_ds) = load_data(FLAGS.data_dir)\n",
    "    train_ds = train_ds.shuffle(60000).batch(FLAGS.batch_size)\n",
    "\n",
    "    # aqui nós criamos a classe contendo o modelo - veja detalhes na seção anterior\n",
    "    model = MNISTModel(data_format)    \n",
    "    # aqui define-se o algoritmo que fará a otmização do modelo\n",
    "    optimizer = tf.train.MomentumOptimizer(FLAGS.lr, FLAGS.momentum)\n",
    "    \n",
    "    # criamos um diretório para armazenar o modelo treinado\n",
    "    if FLAGS.output_dir:\n",
    "        train_dir = os.path.join(FLAGS.output_dir, 'train')\n",
    "        test_dir = os.path.join(FLAGS.output_dir, 'eval')\n",
    "        tf.gfile.MakeDirs(FLAGS.output_dir)\n",
    "    else:\n",
    "        train_dir = None\n",
    "        test_dir = None\n",
    "    # aqui definimos o local no qual o programa armazenará os resumos gerados sobre o treino e sobre o teste\n",
    "    # assim como instância a classe que vai realizar a gravação dos resumos\n",
    "    summary_writer = tf.contrib.summary.create_file_writer(train_dir, flush_millis=10000)\n",
    "    test_summary_writer = tf.contrib.summary.create_file_writer(test_dir, flush_millis=10000, name='test')\n",
    "    checkpoint_prefix = os.path.join(FLAGS.checkpoint_dir, 'ckpt')\n",
    "    \n",
    "    # esta é a parte principal do programa\n",
    "    # primeiro selecionamos o dispositivo que será usado para o treino - CPU ou GPU\n",
    "    with tf.device(device):\n",
    "        # criamos um laço para controlar as épocas\n",
    "        for epoch in range(1, 11):\n",
    "            # a biblioteca nos fornece diversas funções auxiliares pré-construídas para facilitar \n",
    "            # a criação dos modelos. no caso abaixo, utilizamos uma função exclusiva do modo eager execution\n",
    "            # que verifica se temos um modelo já salvo e utiliza para inicializar os parâmetros - pesos - da rede\n",
    "            # isso é bastante útil quando o programa para no meio do treino e precisamos reiniciar a partir de um\n",
    "            # certo ponto. no entanto, se temos um modelo salvo e queremos reiniciar do início, precisamos\n",
    "            # excluir o modelo antigo\n",
    "            with tfe.restore_variables_on_create(tf.train.latest_checkpoint(FLAGS.checkpoint_dir)):\n",
    "                #  esta linha serve para ativar ou buscar um contador interno da biblioteca que retorna o número do \n",
    "                # 'batch' executado - se existe, busca o existente, caso contrário cria um novo\n",
    "                global_step = tf.train.get_or_create_global_step()\n",
    "                start = time.time()\n",
    "                # utilizando a classe que grava os resumos do treino\n",
    "                with summary_writer.as_default():\n",
    "                    # executamos uma época de treino - ver código das funções auxiliares\n",
    "                    train_one_epoch(model, optimizer, train_ds, FLAGS.log_interval)\n",
    "                end = time.time()\n",
    "                # feito isso, imprimimos algumas informações na tela para facilitar o acompanhamento\n",
    "                print('\\nTrain time for epoch #%d (global step %d): %f' % (\n",
    "                    epoch, global_step.numpy(), end - start))\n",
    "                # utilizando a classe que grava os resumos do teste\n",
    "                with test_summary_writer.as_default():\n",
    "                    # executamos uma época de verificação sobre o dataset de teste\n",
    "                    test(model, test_ds)\n",
    "                    # criamos uma lista com todos os parâmetros do modelo (e do laço de treino) que estamos treinando\n",
    "                    all_variables = (model.variables + optimizer.variables() + [global_step])\n",
    "                    # utilizando mais uma função utilitária (exclusiva do modo eager execution),\n",
    "                    # salvamos em disco os parâmetros do modelo que estamos treinando\n",
    "                    tfe.Saver(all_variables).save(checkpoint_prefix, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 - Treinando a rede neural\n",
    "\n",
    "\n",
    "Abaixo temos o código que vai instanciar o programa de treino e otimizar a rede neural. Cada ```parser.add_argument``` adiciona um parâmetro a função do código que vai executar o treino da rede neural (não confundir com os parâmetros - pesos - da rede neural em si).\n",
    "\n",
    "*Observação:* O código abaixo foi feito para rodar em um console python e não em um notebook. Por isso, se precisarmos alterar algum dos parâmetros devemos fazê-lo diretamente no código que adiciona aquele parâmetro.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    '--data-dir',\n",
    "    type=str,\n",
    "    default='/tmp/tensorflow/mnist/input_data',\n",
    "    help='Directory for storing input data')\n",
    "parser.add_argument(\n",
    "    '--batch-size',\n",
    "    type=int,\n",
    "    default=64,\n",
    "    metavar='N',\n",
    "    help='input batch size for training (default: 64)')\n",
    "parser.add_argument(\n",
    "    '--log-interval',\n",
    "    type=int,\n",
    "    default=10,\n",
    "    metavar='N',\n",
    "    help='how many batches to wait before logging training status')\n",
    "parser.add_argument(\n",
    "    '--output_dir',\n",
    "    type=str,\n",
    "    default=None,\n",
    "    metavar='N',\n",
    "    help='Directory to write TensorBoard summaries')\n",
    "parser.add_argument(\n",
    "    '--checkpoint_dir',\n",
    "    type=str,\n",
    "    default='/tmp/tensorflow/mnist/checkpoints/',\n",
    "    metavar='N',\n",
    "    help='Directory to save checkpoints in (once per epoch)')\n",
    "parser.add_argument(\n",
    "    '--lr',\n",
    "    type=float,\n",
    "    default=0.01,\n",
    "    metavar='LR',\n",
    "    help='learning rate (default: 0.01)')\n",
    "parser.add_argument(\n",
    "    '--momentum',\n",
    "    type=float,\n",
    "    default=0.5,\n",
    "    metavar='M',\n",
    "    help='SGD momentum (default: 0.5)')\n",
    "parser.add_argument(\n",
    "    '--no-gpu',\n",
    "    action='store_true',\n",
    "    default=False,\n",
    "    help='disables GPU usage even if a GPU is available')\n",
    "\n",
    "FLAGS, unparsed = parser.parse_known_args()\n",
    "tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 - Desafios\n",
    "\n",
    "Para exercitarmos a utilização da biblioteca, vamos propor alguns desafios:\n",
    "\n",
    "1. Adicione/remova camadas Conv2d/Dense e note como os resultados da classificação mudam.\n",
    "2. Utilize um algoritmo de treino diferente (sugestões: Adam, Adadelta ou Adagrad) e note como o valor de 'Loss' se comporta com estes algoritmos.\n",
    "3. Usando o dataset de teste, implemente o algoritmo de paciência. *Observação*: na prática, nós usamos um dataset de validação para implementar o algoritmo de paciência. \n",
    "4. Acesse a documentação [tf.layers](https://www.tensorflow.org/api_docs/python/tf/layers) e tente adicionar conexões residuais (*residual connections*) e normalização de batch (*batch normalization*) ao seu modelo.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
